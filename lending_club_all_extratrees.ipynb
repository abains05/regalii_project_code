{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting loan grades in Lending Club data set from features similar to those available in Regalii transaction data\n",
    "\n",
    "The overall goal of this analysis was to determine the feasibility of assessing loan risk for Regalii users based on their bill payment history while using the service. The data available from Regalii were logs of each transaction, which included information about the user making the transaction, the transaction amount, the billing account toward which the transaction was being applied, the date of the transaction, and many other features. The first 3 zip code digits of each user were also available in separate data logs. \n",
    "\n",
    "From this transaction log, data was aggregated for each user to determine what features (variables) could be calculated on a per-user basis (see 'regalii_data' notebook). These features included the total number of transactions made, the average amount of each transaction, the number of months since the first bill was paid, the number of billing accounts the user has made payments towards, and several others. \n",
    "\n",
    "However, the transaction log data only contained information about payments made by users- there was no information about charges sent to users before payments occurred, meaning there was no way of telling if a user had paid a bill late or had missed payments. Thus there was no way of accurately measuring how financially disciplined a user was (i.e. Did they miss bills? Could whether they missed a bill be predicted from their past transaction history?). Relating back to the overall goal of the project, this financial discipline measure would be key in deciding if a user was at risk of defaulting on a loan. \n",
    "\n",
    "To bridge this gap, I turned to the publicly available Lending Club data set (https://www.lendingclub.com/info/download-data.action) for loans extended to Lending Club users between 2007-2012. This data contained both loan risk grades assigned to users and certain user features that were roughly similar to those that were available or could be calculated from the Regalii data, including:\n",
    "\n",
    "- Number of total accounts (this meant credit accounts in the Lending Club data but is roughly equated to the number of billing accounts a user made payments on in the Regalii data)\n",
    "- Months since earliest credit line (again, roughly equated to months since the earliest bill payment made by a Regalii user)\n",
    "- Number of open accounts (approximated in the Regalii data by billing accounts towards which a user has made a payment in the last ~1 month)\n",
    "- Revolving credit balance (approximated in Regalii data by total transaction amount in previous month)\n",
    "\n",
    "Also included were features available to Regalii from other data sources (other than transaction logs) that were either privately or publicly accessible, or would be available if a loan was being offered to a user:\n",
    "                     \n",
    "- First 3 zip code digits\n",
    "- Number of missed bill payments (delinquencies) in last 2 years\n",
    "- Total amount of late fees received on bills due\n",
    "- Months since last missed (delinquent) bill payment\n",
    "- Number of derogatory public records\n",
    "- Purpose for loan\n",
    "- Loan amount\n",
    "- Number of credit inquiries in last 6 months\n",
    "- Number of months since last public record\n",
    "- Number of accounts on which the user is now delinquent\n",
    "- Number of public record bankruptcies\n",
    "\n",
    "\n",
    "Initially, these features were all used to try to predict the loan risk grade assigned to each Lending Club user. The purpose of this step was to use the assigned Lending Club loan risk grades as ground truths to see how feasible it was to correctly classify Lending Club users into risk categories using features similar to those available about Regalii users. Additionally, how important each feature was for accruate loan grade classification was also evaluated. This would give some idea to Regalii about how easily they could use currently available information to assign loan risks to users. \n",
    "\n",
    "As a next step, an expanded set of features from the Lending Club data were added to the model to determine whether their inclusion increased loan grade prediction accuracy. These features were chosen because they may be able to be gathered for Regalii users in the future if the company decides to pursue them. They included:\n",
    "\n",
    "- Home ownership (e.g. whether a user rents, has a mortgage, outright owns a home, or otherwise)\n",
    "- Employment length\n",
    "- Annual income\n",
    "- Debt-to-income ratio (could be roughly equated by a ratio of bill payment amounts per month to income level for Regalii users) \n",
    "- Verification status (whether income was verified)\n",
    "\n",
    "If any of these additional features were important in increasing loan grade classification accuracy, these were recommended as new user features that Regalii may be interested in obtaining. \n",
    "\n",
    "\n",
    "# Results summary\n",
    "\n",
    "Results using initial group of features:\n",
    "- \n",
    "\n",
    "\n",
    "The remainder of this notebook consists of the code that was used to obtain these results, including details about the Extra Trees classification model (from the scikit-learn Python library) used to predict loan grade classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amar/anaconda/envs/my_projects_env/lib/python2.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "#Make sure any plots are shown inline in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "#import libraries\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import sklearn.ensemble as ske\n",
    "import sklearn.preprocessing as skp\n",
    "import math\n",
    "import sigopt\n",
    "import seaborn as sb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amar/anaconda/envs/my_projects_env/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#Load data set for accepted loans (it is assumed 'LoanStats3a.csv' was downloaded from the Lending Club \n",
    "#site https://www.lendingclub.com/info/download-data.action (use 2007-2011 loan data; do not use declined \n",
    "#loan data) and placed in the same folder as this Jupyter notebook).\n",
    "loans = pd.read_csv('LoanStats3a.csv', header=1, index_col=0)\n",
    "\n",
    "#Set options so that all columns can be displayed if needed\n",
    "pd.options.display.max_columns = len(loans.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove last two rows that represent summary data\n",
    "l = len(loans.index)\n",
    "loans.drop(loans.index[[l-2,l-1]], inplace=True)\n",
    "del l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Keep only the columns that are to be explored for the model\n",
    "loans = loans.loc[:,['home_ownership',\n",
    "                     'emp_length',\n",
    "                     'annual_inc',\n",
    "                     'verification_status',\n",
    "                     'purpose',\n",
    "                     'earliest_cr_line',\n",
    "#                      'inq_last_12m', #note that all values are Null for this feature; do not include\n",
    "                     'inq_last_6mths',\n",
    "                     'mths_since_last_record',\n",
    "                     'open_acc',\n",
    "                     'revol_bal',\n",
    "                     'total_acc',\n",
    "                     'acc_now_delinq',\n",
    "                     'pub_rec_bankruptcies',\n",
    "                     'delinq_2yrs',\n",
    "                     'dti',\n",
    "                     'loan_amnt',\n",
    "                     'grade',\n",
    "                     'mths_since_last_delinq',\n",
    "                     'pub_rec',\n",
    "                     'total_rec_late_fee',\n",
    "                     'zip_code']]\n",
    "\n",
    "#Check how many non-missing feature values each user has\n",
    "nonnull_count = loans.count(axis=1)\n",
    "\n",
    "#Drop users that have less than len(loans.columns)-1 features with values (turned out that no users had <20 of the 21 features present)\n",
    "loans.drop(nonnull_count.index[nonnull_count.values < len(loans.columns)-1], inplace=True)\n",
    "nonnull_count.drop(nonnull_count.index[nonnull_count.values < len(loans.columns)-1], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of users missing each feature among users missing 0-1 features:\n",
      "home_ownership             0.000000\n",
      "emp_length                 0.000000\n",
      "annual_inc                 0.000000\n",
      "verification_status        0.000000\n",
      "purpose                    0.000000\n",
      "earliest_cr_line           0.000000\n",
      "inq_last_6mths             0.000000\n",
      "mths_since_last_record    78.382379\n",
      "open_acc                   0.000000\n",
      "revol_bal                  0.000000\n",
      "total_acc                  0.000000\n",
      "acc_now_delinq             0.000000\n",
      "pub_rec_bankruptcies       7.833501\n",
      "delinq_2yrs                0.000000\n",
      "dti                        0.000000\n",
      "loan_amnt                  0.000000\n",
      "grade                      0.000000\n",
      "mths_since_last_delinq     7.596661\n",
      "pub_rec                    0.000000\n",
      "total_rec_late_fee         0.000000\n",
      "zip_code                   0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#For the remaining users with 0-1 missing features, show how frequently each feature is missing.\n",
    "#'mths_since_last_record' is normally missing for many users; this is assumed to mean those users \n",
    "#do not have any found public records.\n",
    "missing_feature_perc = 100*(1 - loans.count(axis=0)/len(loans)) #loans.count(axis=0) counts all non-NaN entries for each feature\n",
    "print('% of users missing each feature among users missing 0-1 features:')\n",
    "print(missing_feature_perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Check users who have \"mths_since_last_delinq\" value >24 months or None also \n",
      " all have 0 values for \"delinq_2yrs\" when summed across users:\n",
      "delinq_2yrs    0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Double check that 'delinq_2yrs' is 0 (as expected) for users whose have NaN for 'mths_since_last_delinq' \n",
    "#or a value >24 months. It is assumed users who have NaN have no delinquencies.\n",
    "sample12 = loans[np.logical_or(loans.mths_since_last_delinq.isnull(), loans.mths_since_last_delinq > 24)]\n",
    "print()\n",
    "print('Check users who have \"mths_since_last_delinq\" value >24 months or None also','\\n','all have 0 values for \"delinq_2yrs\" when summed across users:')\n",
    "print(sample12.loc[:,['delinq_2yrs']].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert 'earliest_cr_line' feature to number of months since earliest credit line ('mths_since_earliest_cr_line')\n",
    "\n",
    "#Add '20' or '19' in front of each year in data so pandas can correctly convert to Timestamp\n",
    "dates = list(loans.loc[:,'earliest_cr_line'].values)\n",
    "for i, date in enumerate(dates):\n",
    "    if int(date[-2]) < 2:\n",
    "        dates[i] = date[:-2] + '20' + date[-2:]\n",
    "    elif int(date[-2]) >= 2:\n",
    "        dates[i] = date[:-2] + '19' + date[-2:]\n",
    "\n",
    "#Convert dates to number of months prior to June 2016\n",
    "tdiff = pd.Timestamp('06-01-2016') - pd.to_datetime(pd.Series(dates))\n",
    "tdiff = tdiff.astype('timedelta64[M]')\n",
    "\n",
    "#Add 'mths_since_earliest_cr_line' columns in 'loans' and remove 'earliest_cr_line'\n",
    "loans['mths_since_earliest_cr_line'] = tdiff.values\n",
    "loans.drop('earliest_cr_line', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This cell will plot histograms of ALL features- only run if needed\n",
    "'''\n",
    "#Plot histograms of distributions of data in each remaining column \n",
    "#to make sure there are a variety of values in each column\n",
    "columns = list(loans)\n",
    "for i, column in enumerate(columns):\n",
    "    plt.figure()\n",
    "    if column not in list(loans.select_dtypes(include=['number'])):\n",
    "        loans.loc[:,column].value_counts().plot(kind='bar', title=column)\n",
    "    else:\n",
    "        loans.loc[:,column].plot(kind='hist', title=column, bins=30)\n",
    "    \n",
    "del columns\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set the \"mths_since_last_delinq\" and \"mths_since_last_record\" to 12000 where\n",
    "#they are currently NaN so that they aren't treated as missing data in the model\n",
    "#(these users are assumed to have no delinquencies or public records, so simply \n",
    "#set these NaNs to a very high number of months).\n",
    "loans.loc[np.isnan(loans.mths_since_last_delinq),'mths_since_last_delinq'] = 1200\n",
    "loans.loc[np.isnan(loans.mths_since_last_record),'mths_since_last_record'] = 1200\n",
    "\n",
    "#Fill any NaNs in remaining columns containing missing values with values appropriate for each feature\n",
    "loans = loans.fillna({'pub_rec_bankruptcies': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Shuffle row indices \n",
    "shuffled_index = list(loans.index)\n",
    "np.random.shuffle(shuffled_index)\n",
    "\n",
    "#Use first 10% of shuffled row indices to specify rows for a test set and the remaining 90% to specify \n",
    "#rows for a training set\n",
    "split_ind = int(np.around(0.1*len(loans.index)))\n",
    "all_feat = [ 'home_ownership',\n",
    "             'emp_length',\n",
    "             'annual_inc',\n",
    "             'dti',\n",
    "             'verification_status',\n",
    "             'purpose',\n",
    "             'mths_since_earliest_cr_line',\n",
    "             'inq_last_6mths',\n",
    "             'mths_since_last_record',\n",
    "             'open_acc',\n",
    "             'revol_bal',\n",
    "             'total_acc',\n",
    "             'acc_now_delinq',\n",
    "             'pub_rec_bankruptcies',\n",
    "             'delinq_2yrs',\n",
    "             'loan_amnt',\n",
    "             'grade',\n",
    "             'mths_since_last_delinq',\n",
    "             'pub_rec',\n",
    "             'total_rec_late_fee',\n",
    "             'zip_code']\n",
    "                                          \n",
    "test_set_all = loans.loc[shuffled_index[0:split_ind],all_feat]\n",
    "train_set_all = loans.loc[shuffled_index[split_ind:],all_feat]\n",
    "\n",
    "#Get subset of features corresponding to Regalii's limited feature set\n",
    "limited_feat = [ \n",
    "             'purpose',\n",
    "             'mths_since_earliest_cr_line',\n",
    "             'inq_last_6mths',\n",
    "             'mths_since_last_record',\n",
    "             'open_acc',\n",
    "             'revol_bal',\n",
    "             'total_acc',\n",
    "             'acc_now_delinq',\n",
    "             'pub_rec_bankruptcies',\n",
    "             'delinq_2yrs',\n",
    "             'loan_amnt',\n",
    "             'grade',\n",
    "             'mths_since_last_delinq',\n",
    "             'pub_rec',\n",
    "             'total_rec_late_fee',\n",
    "             'zip_code']\n",
    "test_set_limited = loans.loc[shuffled_index[0:split_ind],limited_feat]\n",
    "train_set_limited = loans.loc[shuffled_index[split_ind:],limited_feat]\n",
    "\n",
    "#Separate all-feature data into arrays of features and labels with categorical variables transformed into 1-of-n coding \n",
    "#(ExtraTreesClassifier doesn't accept non-numerical data)\n",
    "lb_labels = skp.LabelEncoder()\n",
    "lb_labels.fit(loans.loc[:,'grade'].values)\n",
    "test_labels_all = lb_labels.transform(test_set_all.loc[:,'grade'].values)\n",
    "train_labels_all = lb_labels.transform(train_set_all.loc[:,'grade'].values)\n",
    "\n",
    "columns = list(test_set_all)\n",
    "test_features_all = np.empty((len(test_set_all.index),1))\n",
    "train_features_all = np.empty((len(train_set_all.index),1))\n",
    "lb_tmp = skp.LabelBinarizer()\n",
    "for column in columns:\n",
    "    if column != 'grade' and column not in list(test_set_all.select_dtypes(include=['number'])):\n",
    "        lb_tmp.fit(loans.loc[:,column].values)\n",
    "        test_features_all = np.concatenate((test_features_all, lb_tmp.transform(test_set_all.loc[:,column].values)), axis=1)\n",
    "        train_features_all = np.concatenate((train_features_all, lb_tmp.transform(train_set_all.loc[:,column].values)), axis=1)\n",
    "    elif column != 'grade' and column in list(test_set_all.select_dtypes(include=['number'])):\n",
    "        tmp = test_set_all.loc[:,column].values\n",
    "        test_features_all = np.concatenate((test_features_all, tmp.reshape((len(tmp),1))), axis=1)\n",
    "        tmp = train_set_all.loc[:,column].values\n",
    "        train_features_all = np.concatenate((train_features_all, tmp.reshape((len(tmp),1))), axis=1)\n",
    "\n",
    "\n",
    "test_features_all = np.delete(test_features_all, 0, 1)\n",
    "train_features_all = np.delete(train_features_all, 0, 1)\n",
    "\n",
    "del lb_tmp, tmp\n",
    "\n",
    "#Separate limited-feature data into arrays of features and labels with categorical variables transformed into 1-of-n coding \n",
    "#(ExtraTreesClassifier doesn't accept non-numerical data)\n",
    "test_labels_limited = lb_labels.transform(test_set_limited.loc[:,'grade'].values)\n",
    "train_labels_limited = lb_labels.transform(train_set_limited.loc[:,'grade'].values)\n",
    "\n",
    "columns = list(test_set_limited)\n",
    "test_features_limited = np.empty((len(test_set_limited.index),1))\n",
    "train_features_limited = np.empty((len(train_set_limited.index),1))\n",
    "lb_tmp = skp.LabelBinarizer()\n",
    "for column in columns:\n",
    "    if column != 'grade' and column not in list(test_set_limited.select_dtypes(include=['number'])):\n",
    "        lb_tmp.fit(loans.loc[:,column].values)\n",
    "        test_features_limited = np.concatenate((test_features_limited, lb_tmp.transform(test_set_limited.loc[:,column].values)), axis=1)\n",
    "        train_features_limited = np.concatenate((train_features_limited, lb_tmp.transform(train_set_limited.loc[:,column].values)), axis=1)\n",
    "    elif column != 'grade' and column in list(test_set_limited.select_dtypes(include=['number'])):\n",
    "        tmp = test_set_limited.loc[:,column].values\n",
    "        test_features_limited = np.concatenate((test_features_limited, tmp.reshape((len(tmp),1))), axis=1)\n",
    "        tmp = train_set_limited.loc[:,column].values\n",
    "        train_features_limited = np.concatenate((train_features_limited, tmp.reshape((len(tmp),1))), axis=1)\n",
    "\n",
    "\n",
    "test_features_limited = np.delete(test_features_limited, 0, 1)\n",
    "train_features_limited = np.delete(train_features_limited, 0, 1)\n",
    "  \n",
    "del shuffled_index, split_ind, tmp, columns, lb_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (all features):  [ 0.30299047  0.32829445  0.31315789  0.31764319  0.31258235]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=True, class_weight='balanced_subsample',\n",
       "           criterion='gini', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "           oob_score=True, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit extra trees classifier to all-feature data\n",
    "extra_all = ske.ExtraTreesClassifier(n_estimators=50, \n",
    "                                    criterion='gini',\n",
    "                                    max_features=None,\n",
    "                                    class_weight='balanced_subsample',\n",
    "                                    bootstrap=True,\n",
    "                                    oob_score=True\n",
    "                                   )\n",
    "cv_scores_all = sk.cross_validation.cross_val_score(extra_all, train_features_all, train_labels_all, cv=5)\n",
    "print('Cross-validation scores (all features): ', cv_scores_all)\n",
    "extra_all.fit(train_features_all, train_labels_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (limited features):  [ 0.3020046   0.32533684  0.29967105  0.31237656  0.30632411]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=True, class_weight='balanced_subsample',\n",
       "           criterion='gini', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "           oob_score=True, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit extra trees classifier to limited-feature data\n",
    "extra_limited = ske.ExtraTreesClassifier(n_estimators=50, \n",
    "                                    criterion='gini',\n",
    "                                    max_features=None,\n",
    "                                    class_weight='balanced_subsample',\n",
    "                                    bootstrap=True,\n",
    "                                    oob_score=True\n",
    "                                   )\n",
    "\n",
    "cv_scores_limited = sk.cross_validation.cross_val_score(extra_limited, train_features_limited, train_labels_limited, cv=5)\n",
    "print('Cross-validation scores (limited features): ', cv_scores_limited)\n",
    "extra_limited.fit(train_features_limited, train_labels_limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#Use SigOpt to try to optimize parameters (NOTE: this is a long process; have tried previously \n",
    "#and did not improve model much so only run this if you're specifically interested in seeing its output)\n",
    "\n",
    "conn = sigopt.Connection(client_token=\"LKJIIUJAOZIXDRGWSMLJCTTKOWLCCKRJYBOEVZTKYJVOOXRN\")\n",
    "\n",
    "# Create an experiment to find hyperparameters for Extra Trees with All features\n",
    "experiment = conn.experiments().create(\n",
    "  name=\"Extra Trees Optimization All Features\",\n",
    "  parameters=[\n",
    "    {'name': 'n_est', 'bounds': {'max': 200, 'min': 30}, 'type': 'int'},\n",
    "    {'name': 'crit', 'categorical_values': [{'enum_index': 1, 'name': 'gini', 'object': 'categorical_value'}, {'enum_index': 2, 'name': 'entropy', 'object': 'categorical_value'}], 'type': 'categorical'},\n",
    "    {'name': 'max_feat', 'bounds': {'max': len(all_feat), 'min': int(np.around(np.sqrt(len(all_feat))))}, 'type': 'int'},\n",
    "    {'name': 'min_samp_split', 'bounds': {'max': 20, 'min': 1}, 'type': 'int'},\n",
    "    {'name': 'min_samp_leaf', 'bounds': {'max': 20, 'min': 1}, 'type': 'int'},\n",
    "  ],\n",
    ")\n",
    "print(\"Created an experiment with id {0}.\".format(experiment.id))\n",
    "\n",
    "# Receive a suggestion from SigOpt and evaluate\n",
    "def evaluate_metric(n_est, crit, max_feat, min_samp_split, min_samp_leaf, trfeatures, trlabels):\n",
    "    opt_extra = ske.ExtraTreesClassifier(n_estimators=n_est, \n",
    "                                          criterion=crit, \n",
    "                                          max_features=max_feat, \n",
    "                                          min_samples_split=min_samp_split, \n",
    "                                          min_samples_leaf=min_samp_leaf, \n",
    "                                          class_weight='balanced_subsample',\n",
    "                                          bootstrap=True,\n",
    "                                          oob_score=True\n",
    "                                         )\n",
    "    opt_extra.fit(trfeatures, trlabels)\n",
    "\n",
    "    return (opt_extra.oob_score_)\n",
    "\n",
    "# In a loop: receive a suggestion, evaluate metric, then report an observation\n",
    "for it in range(100):\n",
    "    suggestion = conn.experiments(experiment.id).suggestions().create()\n",
    "    n_est = suggestion.assignments['n_est']\n",
    "    crit = suggestion.assignments['crit']\n",
    "    max_feat = suggestion.assignments['max_feat']\n",
    "    min_samp_split = suggestion.assignments['min_samp_split']\n",
    "    min_samp_leaf = suggestion.assignments['min_samp_leaf']\n",
    "    \n",
    "    print(\"Received a suggestion with n_est={n_est}, crit={crit}, max_feat={max_feat}, min_samp_split={min_samp_split}, min_samp_leaf={min_samp_leaf}.\".format(n_est=n_est, crit=crit, max_feat=max_feat, min_samp_split=min_samp_split, min_samp_leaf=min_samp_leaf))\n",
    "    value = evaluate_metric(n_est, crit, max_feat, min_samp_split, min_samp_leaf, train_features_all, train_labels_all)\n",
    "    print(\"Iteration {0} complete.\".format(it))\n",
    "    print(\"The function evaluated to {0} using these parameters.\".format(value))\n",
    "    conn.experiments(experiment.id).observations().create(\n",
    "    suggestion=suggestion.id,\n",
    "    value=value,\n",
    "  )\n",
    "  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy if just chose most common training class :  0.261973684211\n",
      "\n",
      "\n",
      "Proportion of correct test predictions (all-feature data):  0.329188869153\n",
      "Train set grade proportions (all-feature data):\n",
      "A :  0.115\n",
      "B :  0.261973684211\n",
      "C :  0.249473684211\n",
      "D :  0.194342105263\n",
      "E :  0.113421052632\n",
      "F :  0.0458552631579\n",
      "G :  0.0199342105263\n",
      "\n",
      "Predicted grade proportions (all-feature data):\n",
      "A :  0.104795737123\n",
      "B :  0.338069863825\n",
      "C :  0.282415630551\n",
      "D :  0.161042036708\n",
      "E :  0.0787448194198\n",
      "F :  0.0236826524571\n",
      "G :  0.0112492599171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amar/anaconda/envs/my_projects_env/lib/python2.7/site-packages/ipykernel/__main__.py:59: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfYAAAFtCAYAAAD1Skg8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD20qOaCxnwBaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11124ea50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Show what the predicted classification accuracy would be if just chose most frequent grade in \n",
    "#the train data as the predicted classification for all test samples. This is a baseline to \n",
    "#which to compare model results.\n",
    "grades = np.unique(loans.loc[:,'grade'].values)\n",
    "grade_counts_all,_ = np.histogram(train_labels_all, bins=np.arange(-0.5,len(grades)+0.5))\n",
    "print('Accuracy if just chose most common training class : ', max(grade_counts_all/grade_counts_all.sum()))\n",
    "print()\n",
    "print()\n",
    "\n",
    "############### ALL-FEATURE PREDICTIONS ################\n",
    "\n",
    "#Use classifier to return accuracy score based on binary correct/incorrect grade classification. \n",
    "print('Proportion of correct test predictions (all-feature data): ',extra_all.score(test_features_all, test_labels_all))\n",
    "\n",
    "#In addition to a score based on binary correct/incorrect grade prediction, also determine score\n",
    "#based on how close grade assignment was to actual grade in all-feature data (e.g. predicted \n",
    "#grade of 'A' when true grade was 'C' would be an error of 2). Show the median of this score.\n",
    "predicted_labels_all = extra_all.predict(test_features_all)\n",
    "\n",
    "#Count how many test samples were at what distance from the correct class and plot histogram for all-feature data\n",
    "prediction_distances_all = np.fabs(test_labels_all-predicted_labels_all)\n",
    "distance_counts_all, bin_edges = np.histogram(prediction_distances_all, bins=np.arange(-0.5,len(grades)+0.5))\n",
    "plt.bar(bin_edges[:len(distance_counts_all)], distance_counts_all, width=1)\n",
    "plt.title('Counts of Predicted Grades Distances from True Grades in Test Set')\n",
    "plt.xlabel('Distance of Predicted Grade from True Grade')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "#Check that the model did not mostly output the most common class in the training data to achieve the given \n",
    "#accuracy level (proportions of different grades predicted grade set should be similar to training data). \n",
    "print('Train set grade proportions (all-feature data):')\n",
    "class_proportions = np.zeros((1,len(distance_counts_all)))\n",
    "for i, grade in enumerate(grades):\n",
    "    class_proportions[0,i] = len([x for x in train_labels_all if x == i])/len(train_labels_all)\n",
    "    print(grade, ': ', class_proportions[0,i])\n",
    "    \n",
    "print()    \n",
    "\n",
    "print('Predicted grade proportions (all-feature data):')\n",
    "for i, grade in enumerate(grades):\n",
    "    print(grade, ': ', len([x for x in predicted_labels_all if x == i])/len(predicted_labels_all))\n",
    "\n",
    "#Determine the error distance counts that would occur if class labels were guessed at random at the\n",
    "#frequency with which each label appeared in the training data\n",
    "guess_joint_prob = np.tile(class_proportions, (len(distance_counts_all),1))*np.tile(class_proportions.T, len(distance_counts_all))\n",
    "random_guess_distance_counts = np.zeros(len(distance_counts_all))\n",
    "for i in range(len(grades)):\n",
    "    for j in range(len(distance_counts_all)):\n",
    "        if i-j >= 0:\n",
    "            random_guess_distance_counts[i-j] += guess_joint_prob[i,j]*len(test_labels_all)\n",
    "        elif j-i > 0:\n",
    "            random_guess_distance_counts[j-i] += guess_joint_prob[i,j]*len(test_labels_all)\n",
    "\n",
    "#Determine the error distance counts that would occur if the most frequent training class label was guessed \n",
    "#in every test case\n",
    "frequent_class_guess_distance_counts = np.zeros(len(distance_counts_all))\n",
    "max_freq_id = np.argmax(class_proportions)\n",
    "for i, grade in enumerate(grades):\n",
    "    dist = np.fabs(i - max_freq_id)\n",
    "    frequent_class_guess_distance_counts[dist] += class_proportions[0][i]*len(test_labels_all)\n",
    "\n",
    "del grade, i, dist, max_freq_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use classifier to return score and output predicted test labels for limited-feature data\n",
    "print('Proportion of correct test predictions (limited-feature data): ',extra_limited.score(test_features_limited, test_labels_limited))\n",
    "predicted_labels_limited = extra_limited.predict(test_features_limited)\n",
    "\n",
    "#Instead of a score based on correct/incorrect class assignment, see how \n",
    "#close class assignment was to actual class for limited-feature data\n",
    "test_grade_limited = lb_labels.inverse_transform(test_labels_limited)\n",
    "predicted_grade_limited = lb_labels.inverse_transform(predicted_labels_limited)\n",
    "\n",
    "grades = np.unique(loans.loc[:,'grade'].values)\n",
    "\n",
    "for i, grade in enumerate(grades):\n",
    "    np.copyto(test_grade_limited, i*np.ones(test_grade_limited.shape), where=test_grade_limited==grade)\n",
    "    \n",
    "for i, grade in enumerate(grades):\n",
    "    np.copyto(predicted_grade_limited, i*np.ones(predicted_grade_limited.shape), where=predicted_grade_limited==grade)\n",
    "\n",
    "test_grade_limited = test_grade_limited.astype('int')\n",
    "predicted_grade_limited = predicted_grade_limited.astype('int')\n",
    "\n",
    "#Show what the predicted classification accuracy would be if just chose most frequent class in \n",
    "#the test data as the predicted classification for limited-feature data\n",
    "grade_counts_limited,_ = np.histogram(test_grade_limited, bins=np.arange(-0.5,len(grades)+0.5))\n",
    "print('Accuracy if just chose most common training class (limited-feature data): ', max(grade_counts_limited/grade_counts_limited.sum()))\n",
    "\n",
    "#Count how many test samples were at what distance from the correct class and plot histogram for limited-feature data\n",
    "prediction_distances_limited = np.fabs(test_grade_limited-predicted_grade_limited)\n",
    "distance_counts_limited, bin_edges = np.histogram(prediction_distances_limited, bins=np.arange(-0.5,len(grades)+0.5))\n",
    "plt.bar(bin_edges[:len(distance_counts_limited)], distance_counts_limited, width=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set class proportions (limited-feature data):\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'distance_counts_limited' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-19aaf054e213>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train set class proportions (limited-feature data):'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrandom_guess_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mclass_proportions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance_counts_limited\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrade\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrades\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mclass_proportions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_grade_limited\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_grade_limited\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'distance_counts_limited' is not defined"
     ]
    }
   ],
   "source": [
    "#Check that the model did not just output the most common class in the training data to achieve the given \n",
    "#accuracy level (proportion of classes in train set and predicted label set should be similar). \n",
    "print('Train set class proportions (limited-feature data):')\n",
    "random_guess_accuracy = 0\n",
    "class_proportions = np.zeros((1,len(distance_counts_limited)))\n",
    "for i, grade in enumerate(grades):\n",
    "    class_proportions[0,i] = len([x for x in train_grade_limited if x == i])/len(train_grade_limited)\n",
    "    print(grade, ': ', class_proportions[0,i])\n",
    "    random_guess_accuracy += class_proportions[0,i]*class_proportions[0,i]\n",
    "\n",
    "print()    \n",
    "\n",
    "print('Predicted label class proportions (limited-feature data):')\n",
    "for i, grade in enumerate(grades):\n",
    "    print(grade, ': ', len([x for x in predicted_grade_limited if x == i])/len(predicted_grade_limited))\n",
    "\n",
    "del grade, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Compare predicted classifications errors between All and Limited models\n",
    "\n",
    "#Confusion matrices\n",
    "test_classes_all = lb_labels.inverse_transform(test_labels_all)\n",
    "predicted_classes_all = lb_labels.inverse_transform(predicted_labels_all)\n",
    "cf = sk.metrics.confusion_matrix(test_classes_all.tolist(), predicted_classes_all.tolist())\n",
    "tmp = np.tile(np.sum(cf, axis=1), (cf.shape[1],1)).T\n",
    "confusion_matrix_all = cf/tmp\n",
    "\n",
    "fig1 = plt.figure()\n",
    "sb.heatmap(confusion_matrix_all, vmin=0, vmax=1, square=True, cmap='Blues', xticklabels=np.unique(test_classes_all.tolist()), yticklabels=np.unique(test_classes_all.tolist()))\n",
    "plt.title('Using All Features')\n",
    "\n",
    "test_classes_limited = lb_labels.inverse_transform(test_labels_limited)\n",
    "predicted_classes_limited = lb_labels.inverse_transform(predicted_labels_limited)\n",
    "cf = sk.metrics.confusion_matrix(test_classes_limited.tolist(), predicted_classes_limited.tolist())\n",
    "tmp = np.tile(np.sum(cf, axis=1), (cf.shape[1],1)).T\n",
    "confusion_matrix_limited = cf/tmp\n",
    "\n",
    "fig2 = plt.figure()\n",
    "sb.heatmap(confusion_matrix_limited, vmin=0, vmax=1, square=True, cmap='Greens', xticklabels=np.unique(test_classes_limited.tolist()), yticklabels=np.unique(test_classes_limited.tolist()))\n",
    "plt.title('Using Limited Features')\n",
    "\n",
    "#Histogram of distance from the correct class\n",
    "d = (prediction_distances_all, prediction_distances_limited)\n",
    "\n",
    "fig3, ax = plt.subplots(figsize=(10,5))\n",
    "plt.hist(d, bins=np.arange(-0.5,7.5))\n",
    "plt.xticks(range(len(np.unique(test_classes_all.tolist()))))\n",
    "plt.xlabel('Distance From Correct Loan Grade')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.legend(['All Features', 'Limited Features'], loc='upper right')\n",
    "\n",
    "#Histogram of distance from the correct class plus the predicted count of each distance if class \n",
    "#predictions were guessed at random based on training class frequencies\n",
    "d = (prediction_distances_all, prediction_distances_limited)\n",
    "\n",
    "fig4, ax = plt.subplots(figsize=(10,5))\n",
    "plt.hist(d, bins=np.arange(-0.5,7.5), rwidth=0.5)\n",
    "plt.bar(np.array([0.25, 0.26, 0.25, 0.25, 0.25, 0.25, 0.25])+range(len(distance_counts_all)), random_guess_distance_counts, width=0.25, color='Purple')\n",
    "plt.xticks(range(len(np.unique(test_classes_all.tolist()))), ['Correct Grade', 'Missed by 1 Grade', 'Missed by 2 Grades', \n",
    "                                                             'Missed by 3 Grades', 'Missed by 4 Grades', \n",
    "                                                              'Missed by 5 Grades', 'Missed by 6 Grades'], rotation=45)\n",
    "plt.xlabel('Distance From Correct Loan Grade')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.legend(['All Features', 'Limited Features', 'Random Guess'], loc='upper right')\n",
    "\n",
    "#Histogram of distance from the correct class plus the predicted count of each distance if most \n",
    "#frequent class in training data was always chosen\n",
    "d = (prediction_distances_all, prediction_distances_limited)\n",
    "\n",
    "fig5, ax = plt.subplots(figsize=(10,5))\n",
    "plt.hist(d, bins=np.arange(-0.5,7.5), rwidth=0.5)\n",
    "plt.bar(np.array([0.25, 0.26, 0.25, 0.25, 0.25, 0.25, 0.25])+range(len(distance_counts_all)), frequent_class_guess_distance_counts, width=0.25, color='Purple')\n",
    "plt.xticks(range(len(np.unique(test_classes_all.tolist()))), ['Correct Grade', 'Missed by 1 Grade', 'Missed by 2 Grades', \n",
    "                                                             'Missed by 3 Grades', 'Missed by 4 Grades', \n",
    "                                                              'Missed by 5 Grades', 'Missed by 6 Grades'], rotation=45)\n",
    "plt.xlabel('Distance From Correct Loan Grade')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.legend(['All Features', 'Limited Features', 'Most Frequent Class Guess'], loc='upper right')\n",
    "\n",
    "del tmp, d, cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct = 0\n",
    "extra_all_importances = np.zeros(len(all_feat))\n",
    "tmp_importances = extra_all.feature_importances_\n",
    "for i, column in enumerate(all_feat):\n",
    "    if column != 'grade' and column in all_feat:\n",
    "        if column != 'grade' and column not in train_set_all.select_dtypes(include=['number']):\n",
    "            extra_all_importances[i] = np.sum(tmp_importances[ct:len(train_set_all.loc[:,column].unique())])\n",
    "            ct += len(train_set_all.loc[:,column].unique())\n",
    "        elif column != 'grade' and column in train_set_all.select_dtypes(include=['number']):\n",
    "            extra_all_importances[i] = tmp_importances[ct]\n",
    "            ct += 1\n",
    "\n",
    "ct = 0\n",
    "extra_limited_importances = np.zeros(len(all_feat))\n",
    "tmp_importances = extra_limited.feature_importances_\n",
    "for i, column in enumerate(all_feat):\n",
    "    if column != 'grade' and column in limited_feat:\n",
    "        if column != 'grade' and column not in train_set_limited.select_dtypes(include=['number']):\n",
    "            extra_limited_importances[i] = np.sum(tmp_importances[ct:len(train_set_limited.loc[:,column].unique())])\n",
    "            ct += len(train_set_limited.loc[:,column].unique())\n",
    "        elif column != 'grade' and column in train_set_limited.select_dtypes(include=['number']):\n",
    "            extra_limited_importances[i] = tmp_importances[ct]\n",
    "        ct += 1\n",
    "\n",
    "print(extra_all_importances)\n",
    "print(extra_limited_importances)\n",
    "\n",
    "del tmp_importances, ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(np.cumsum(distance_counts_all))\n",
    "print(np.cumsum(distance_counts_limited))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sb.set(font_scale=2)\n",
    "d = {'All': extra_all_importances, 'Limited': extra_limited_importances}\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "# Setting the positions and width for the bars\n",
    "pos = list(range(len(df['All'])))\n",
    "width = 0.3\n",
    "\n",
    "# Plotting the bars\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "# Create a bar with All feature importance data,\n",
    "# in position pos,\n",
    "plt.bar(pos, df['All'], width, alpha=0.5, color='#EE3224')\n",
    "\n",
    "# Create a bar with Limited feature importance data,\n",
    "# in position pos + some width buffer,\n",
    "plt.bar([p + width for p in pos], df['Limited'], width, alpha=0.5, color='#F78F1E')\n",
    "\n",
    "# Set the y axis label\n",
    "ax.set_ylabel('Normalized Feature Importance')\n",
    "\n",
    "# Set the chart's title\n",
    "ax.set_title('Extra Trees Gini Feature Importance')\n",
    "\n",
    "# Set the position of the x ticks\n",
    "ax.set_xticks([p + width for p in pos])\n",
    "\n",
    "# Set the labels for the x ticks\n",
    "ax.set_xticklabels(list(all_feat), rotation=90)\n",
    "\n",
    "# Setting the x-axis and y-axis limits\n",
    "plt.xlim(min(pos)-width, max(pos)+width*3)\n",
    "plt.ylim([0, max(max(df['All']),max(df['Limited']))+0.02] )\n",
    "\n",
    "# Adding the legend and showing the plot\n",
    "plt.legend(['Extended Features', 'Limited Features'], loc='upper left')\n",
    "\n",
    "del d, df, pos, width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Calculate classes by weighting the predicted probability of each class \n",
    "#(instead of the class with the highest probability)\n",
    "class_prob_all = extra_all.predict_proba(test_features_all)\n",
    "weighted_labels_all = np.zeros((np.shape(class_prob_all)[0]))\n",
    "class_labels = range(np.shape(class_prob_all)[1])\n",
    "for i, sample in enumerate(class_prob_all):\n",
    "    weighted_labels_all[i] = np.round(np.sum(sample*class_labels))\n",
    "weighted_labels_all = weighted_labels_all.astype(int, copy=False)\n",
    "\n",
    "class_prob_limited = extra_limited.predict_proba(test_features_limited)\n",
    "weighted_labels_limited = np.zeros((np.shape(class_prob_limited)[0]))\n",
    "class_labels = range(np.shape(class_prob_limited)[1])\n",
    "for i, sample in enumerate(class_prob_limited):\n",
    "    weighted_labels_limited[i] = np.round(np.sum(sample*class_labels))\n",
    "weighted_labels_limited = weighted_labels_limited.astype(int, copy=False)\n",
    "\n",
    "del class_labels\n",
    "\n",
    "#Confusion matrices\n",
    "sb.set(font_scale=2)\n",
    "\n",
    "# fig1 = plt.figure(frameon=False)\n",
    "# sb.heatmap(confusion_matrix_all, vmin=0, vmax=1, square=True, cmap='Blues', xticklabels=np.unique(grades.tolist()), yticklabels=np.unique(grades.tolist()))\n",
    "# plt.title('Using All Features: Max Probability Class Assignment')\n",
    "\n",
    "cf = sk.metrics.confusion_matrix(test_labels_all.tolist(), weighted_labels_all.tolist())\n",
    "tmp = np.tile(np.sum(cf, axis=1), (cf.shape[1],1)).T\n",
    "confusion_matrix_weighted_all = cf/tmp\n",
    "\n",
    "fig2 = plt.figure(frameon=False)\n",
    "sb.heatmap(confusion_matrix_weighted_all, vmin=0, vmax=1, square=True, cmap='Blues', xticklabels=np.unique(grades.tolist()), yticklabels=np.unique(grades.tolist()))\n",
    "# plt.title('Using All Features: Probability-Weighted Class Assignment')\n",
    "plt.xlabel('Predicted Grade')\n",
    "plt.ylabel('True Grade')\n",
    "\n",
    "# fig3 = plt.figure(frameon=False)\n",
    "# sb.heatmap(confusion_matrix_limited, vmin=0, vmax=1, square=True, cmap='Greens', xticklabels=np.unique(grades.tolist()), yticklabels=np.unique(grades.tolist()))\n",
    "# plt.title('Using Limited Features: Max Probability Class Assignment')\n",
    "\n",
    "cf = sk.metrics.confusion_matrix(test_labels_limited.tolist(), weighted_labels_limited.tolist())\n",
    "tmp = np.tile(np.sum(cf, axis=1), (cf.shape[1],1)).T\n",
    "confusion_matrix_weighted_limited = cf/tmp\n",
    "\n",
    "fig4 = plt.figure(frameon=False)\n",
    "sb.heatmap(confusion_matrix_weighted_limited, vmin=0, vmax=1, square=True, cmap='Greens', xticklabels=np.unique(grades.tolist()), yticklabels=np.unique(grades.tolist()))\n",
    "# plt.title('Using Limited Features: Probability-Weighted Class Assignment')\n",
    "plt.xlabel('Predicted Grade')\n",
    "plt.ylabel('True Grade')\n",
    "\n",
    "#Count how many test samples were at what distance from the correct class \n",
    "weighted_distances_all = np.fabs(test_grade_all-weighted_labels_all)\n",
    "weighted_distance_counts_all, bin_edges = np.histogram(weighted_distances_all, bins=np.arange(-0.5,len(grades)+0.5))\n",
    "\n",
    "weighted_distances_limited = np.fabs(test_grade_limited-weighted_labels_limited)\n",
    "weighted_distance_counts_limited, bin_edges = np.histogram(weighted_distances_limited, bins=np.arange(-0.5,len(grades)+0.5))\n",
    "\n",
    "#Histograms of distance from the correct class plus the predicted count of each distance if most \n",
    "#frequent class in training data was always chosen\n",
    "d = (weighted_distances_all, weighted_distances_limited)\n",
    "\n",
    "# fig5, ax = plt.subplots(figsize=(10,5))\n",
    "# plt.bar(np.array([0.25, 0.26, 0.25, 0.25, 0.25, 0.25, 0.25])+range(len(distance_counts_all)), frequent_class_guess_distance_counts, width=0.25, color='Purple')\n",
    "# plt.xticks(range(len(np.unique(test_classes_all.tolist()))), ['Correct Grade', 'Missed by 1 Grade', 'Missed by 2 Grades', \n",
    "#                                                              'Missed by 3 Grades', 'Missed by 4 Grades', \n",
    "#                                                               'Missed by 5 Grades', 'Missed by 6 Grades'], rotation=45)\n",
    "# ax.patch.set_facecolor('none')\n",
    "# plt.xlabel('Distance From Correct Loan Grade')\n",
    "# plt.ylabel('Number of Users')\n",
    "# plt.legend(['Most Frequent Class Guess'], loc='upper right')\n",
    "\n",
    "\n",
    "# fig6, ax = plt.subplots(figsize=(10,5))\n",
    "# plt.hist((weighted_distances_all, [0]), bins=np.arange(-0.5,7.5), rwidth=0.5)\n",
    "# plt.bar(np.array([0.25, 0.26, 0.25, 0.25, 0.25, 0.25, 0.25])+range(len(distance_counts_all)), frequent_class_guess_distance_counts, width=0.25, color='Purple')\n",
    "# plt.xticks(range(len(np.unique(test_classes_all.tolist()))), ['Correct Grade', 'Missed by 1 Grade', 'Missed by 2 Grades', \n",
    "#                                                              'Missed by 3 Grades', 'Missed by 4 Grades', \n",
    "#                                                               'Missed by 5 Grades', 'Missed by 6 Grades'], rotation=45)\n",
    "# ax.patch.set_facecolor('none')\n",
    "# plt.xlabel('Distance From Correct Loan Grade')\n",
    "# plt.ylabel('Number of Users')\n",
    "# plt.legend(['All Features - Probability-Weighted Class Assignment', 'Limited Features - Probability-Weighted Class Assignment', 'Most Frequent Class Guess'], loc='upper right')\n",
    "\n",
    "fig7, ax = plt.subplots(figsize=(10,5))\n",
    "plt.hist(d, bins=np.arange(-0.5,7.5), rwidth=0.5)\n",
    "plt.bar(np.array([0.25, 0.26, 0.25, 0.25, 0.25, 0.25, 0.25])+range(len(distance_counts_all)), frequent_class_guess_distance_counts, width=0.25, color='Purple')\n",
    "plt.xticks(range(len(np.unique(test_classes_all.tolist()))), ['Correct Grade', 'Missed by 1 Grade', 'Missed by 2 Grades', \n",
    "                                                             'Missed by 3 Grades', 'Missed by 4 Grades', \n",
    "                                                              'Missed by 5 Grades', 'Missed by 6 Grades'], rotation=45)\n",
    "ax.patch.set_facecolor('none')\n",
    "plt.xlabel('Distance From Correct Loan Grade')\n",
    "plt.ylabel('Number of Users')\n",
    "# plt.legend(['Expanded Features - Probability-Weighted Class Assignment', 'Limited Features - Probability-Weighted Class Assignment', 'Most Frequent Class Guess'], loc='upper right')\n",
    "plt.legend(['Expanded Features', 'Limited Features', 'Most Frequent Class Guess'], loc='upper right')\n",
    "\n",
    "del tmp, d, cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(prediction_distances_all.mean())\n",
    "print(weighted_distances_all.mean())\n",
    "print(prediction_distances_limited.mean())\n",
    "print(weighted_distances_limited.mean())\n",
    "print(np.sum(frequent_class_guess_distance_counts*range(len(grades)))/np.sum(frequent_class_guess_distance_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate proportions of test data in each error distance bin and show cumulative sums\n",
    "\n",
    "#Guessing most frequent category\n",
    "weighted_distances_limited_proportions, _ = np.histogram(weighted_distances_limited, bins=np.arange(-0.5,7.5))\n",
    "print('Cumulative error distance proportions, frequent class guess: ')\n",
    "print(np.cumsum(frequent_class_guess_distance_counts/np.sum(frequent_class_guess_distance_counts)))\n",
    "print()\n",
    "\n",
    "#All features\n",
    "weighted_distances_all_proportions, _ = np.histogram(weighted_distances_all, bins=np.arange(-0.5,7.5))\n",
    "print('Cumulative error distance proportions, all features: ')\n",
    "print(np.cumsum(weighted_distances_all_proportions/len(weighted_distances_all)))\n",
    "print()\n",
    "\n",
    "#Limited features\n",
    "weighted_distances_limited_proportions, _ = np.histogram(weighted_distances_limited, bins=np.arange(-0.5,7.5))\n",
    "print('Cumulative error distance proportions, limited features: ')\n",
    "print(np.cumsum(weighted_distances_limited_proportions/len(weighted_distances_limited)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
